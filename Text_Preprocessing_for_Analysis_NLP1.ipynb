{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Getting Started With NLTK:**"
      ],
      "metadata": {
        "id": "GTeA8mshW-vc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our first step is to install NLTK with pip."
      ],
      "metadata": {
        "id": "xWmb21zURLyP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDAEHXnSQ1Xv",
        "outputId": "b337c16f-71d6-4c4e-9852-3e26db40ad0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.5 in /usr/local/lib/python3.10/dist-packages (3.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.5) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.5) (1.3.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from nltk==3.5) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.5) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk==3.5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to install NumPy and Matplotlib."
      ],
      "metadata": {
        "id": "ef2MuIq6Regn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggE_LOUmRWtX",
        "outputId": "2f63b16a-8f09-4cdf-802e-8f46f7365631"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizing:**\n",
        "Using tokenizing, we can split up text by word or by sentence. It’s an initial step to convert the unstructured data into structured data for easier analysis purposes.\n",
        "\n",
        "We shall be tokenizing by word and tokenizing by sentence."
      ],
      "metadata": {
        "id": "uUaJGWK2Yg9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Tokenizing by word:** Words are like the atoms of natural language. These are the smallest units of meaning that still make sense on their own."
      ],
      "metadata": {
        "id": "2V6DoYtIXq9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Tokenizing by sentence:** When we tokenize by sentence, we can analyze how those words relate to one another and see more context."
      ],
      "metadata": {
        "id": "_LWoTtajX2OY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we shall import the relevant parts of NLTK so you can tokenize by word and by sentence."
      ],
      "metadata": {
        "id": "Kj9EhlGrSOMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "id": "AirtXoD5SMBQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can create a string to tokenize."
      ],
      "metadata": {
        "id": "BrgEmXnoSydB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_string = \"\"\"\n",
        "... Muad'Dib learned rapidly because his first training was in how to learn.\n",
        "... And the first lesson of all was the basic trust that he could learn.\n",
        "... It's shocking to find how many people do not believe they can learn,\n",
        "... and how many more believe learning to be difficult.\"\"\""
      ],
      "metadata": {
        "id": "92wQ2vzCSv8Q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj9H5dJbUNoq",
        "outputId": "0f7be515-4064-4008-b1f4-1b71d43a7155"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use sent_tokenize() to split up example_string into sentences."
      ],
      "metadata": {
        "id": "eHWs7MWoUfwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(example_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPwU-ofNTvyy",
        "outputId": "2ba96013-746d-46ba-ddd8-694aeeb5d78e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"\\nMuad'Dib learned rapidly because his first training was in how to learn.\",\n",
              " 'And the first lesson of all was the basic trust that he could learn.',\n",
              " \"It's shocking to find how many people do not believe they can learn,\\nand how many more believe learning to be difficult.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing example_string by sentence provides us with a list of three strings that are sentences:\n",
        "\n",
        "1. \"Muad'Dib learned rapidly because his first training was in how to learn.\"\n",
        "2. 'And the first lesson of all was the basic trust that he could learn.'\n",
        "3. \"It's shocking to find how many people do not believe they can learn, and how many more believe learning to be difficult.\""
      ],
      "metadata": {
        "id": "u9HImCPfVCM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are tokenizing example_string by word."
      ],
      "metadata": {
        "id": "DF7y24_WU5Y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(example_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSG5fw9RUuN6",
        "outputId": "3fef405f-3720-4abe-d6bd-d90dec736f35"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Muad'Dib\",\n",
              " 'learned',\n",
              " 'rapidly',\n",
              " 'because',\n",
              " 'his',\n",
              " 'first',\n",
              " 'training',\n",
              " 'was',\n",
              " 'in',\n",
              " 'how',\n",
              " 'to',\n",
              " 'learn',\n",
              " '.',\n",
              " 'And',\n",
              " 'the',\n",
              " 'first',\n",
              " 'lesson',\n",
              " 'of',\n",
              " 'all',\n",
              " 'was',\n",
              " 'the',\n",
              " 'basic',\n",
              " 'trust',\n",
              " 'that',\n",
              " 'he',\n",
              " 'could',\n",
              " 'learn',\n",
              " '.',\n",
              " 'It',\n",
              " \"'s\",\n",
              " 'shocking',\n",
              " 'to',\n",
              " 'find',\n",
              " 'how',\n",
              " 'many',\n",
              " 'people',\n",
              " 'do',\n",
              " 'not',\n",
              " 'believe',\n",
              " 'they',\n",
              " 'can',\n",
              " 'learn',\n",
              " ',',\n",
              " 'and',\n",
              " 'how',\n",
              " 'many',\n",
              " 'more',\n",
              " 'believe',\n",
              " 'learning',\n",
              " 'to',\n",
              " 'be',\n",
              " 'difficult',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we got a list of strings that NLTK considers to be words, such as: \"Muad'Dib\" ,  'training' , 'how' and so on.\n",
        "\n",
        "We also had the following strings that were also considered words: \" 's \" ,  ' , ' and ' . '."
      ],
      "metadata": {
        "id": "wmRspqCCV3gp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can note that \"Muad'Dib\" isn’t an accepted contraction like \"It's\" as we know that \"'s\" is a contraction of \"is\". Therefore, it wasn’t read as two separate words and was left intact."
      ],
      "metadata": {
        "id": "CETV-WnsX08Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Filtering Stop Words:** We filter out the stop words like 'in', 'is', and 'an', as they are not important in the sentence."
      ],
      "metadata": {
        "id": "2zNOYdCUg0f_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To filter the stop words, import the relevant parts of NLTK."
      ],
      "metadata": {
        "id": "6yedD34zh3lG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCFgeha6VfSe",
        "outputId": "61af2084-f99f-45c6-cba0-aaa7bdd2d21d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "Mx1eBst0YA6n"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "DbNSsl1EYPp6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us filter the following quote."
      ],
      "metadata": {
        "id": "_QOek1StiGiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "worf_quote = \"Sir, I protest. I am not a merry man!\""
      ],
      "metadata": {
        "id": "LxhTlpNqf8SI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After tokenizing worf_quote by word, we can store the resulting list in words_in_quote."
      ],
      "metadata": {
        "id": "klpGD8EFisOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_quote = word_tokenize(worf_quote)"
      ],
      "metadata": {
        "id": "BY1wHhLFgB-4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_quote"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9TuM0P-gD84",
        "outputId": "63632297-5668-4456-e47d-0cb99fc07c5b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sir', ',', 'I', 'protest', '.', 'I', 'am', 'not', 'a', 'merry', 'man', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We focused on stop words in \"english\"."
      ],
      "metadata": {
        "id": "F_zaBxTEYhNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "0JqDHLSkgI9v"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we have created an empty list to hold the words that make it past the filter."
      ],
      "metadata": {
        "id": "mv7L4kL1l0r5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_list = []"
      ],
      "metadata": {
        "id": "jgkQHaoWgRe3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use stop_words to filter words_in_quote."
      ],
      "metadata": {
        "id": "tiobgM0ybaY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words_in_quote:\n",
        "...    if word.casefold() not in stop_words:\n",
        "...         filtered_list.append(word)"
      ],
      "metadata": {
        "id": "dOwO4hCzgddS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We iterated over words_in_quote with a for loop and added all the words that were not stop words to the filtered_list. We used .casefold() on word so we could ignore whether the letters in the word were uppercase or lowercase. This is worth doing because stopwords.words('english') include only lowercase versions of stop words.\n",
        "\n",
        "Alternatively, we could use list comprehension to make a list of all the words in our text that are not stop words:"
      ],
      "metadata": {
        "id": "6eim4G7qcW-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_list = [\n",
        "...     word for word in words_in_quote if word.casefold() not in stop_words\n",
        "... ]"
      ],
      "metadata": {
        "id": "MpIFAd_GgfW4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we got the following filtered_list."
      ],
      "metadata": {
        "id": "Xd-cmqsvcvGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv9lfebvgl1n",
        "outputId": "a61fff6f-1c7e-45ac-e2ac-5d2a7777a882"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sir', ',', 'protest', '.', 'merry', 'man', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We filtered out words like 'am' and 'a' from the sentence. The pronoun 'I' and adverb 'not' are not important parts of a sentence; that's why they are removed from the sentence."
      ],
      "metadata": {
        "id": "bSKwrFfJkl0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming:** Stemming is a text processing task in which we reduce words to their root, which is the core part of a word."
      ],
      "metadata": {
        "id": "bSjQ6zjU0SAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we import the relevant parts of NLTK in order to start stemming."
      ],
      "metadata": {
        "id": "0w9Vp3yP0E9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "-ZK3tv8AyHMk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "qlcCYfW0yLQk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can create a stemmer with PorterStemmer()."
      ],
      "metadata": {
        "id": "Nt5QHVtxz0A0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "qxzLfU0lyPyc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We created a string for the stem."
      ],
      "metadata": {
        "id": "kt1vnQ-Gzl0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string_for_stemming = \"\"\"\n",
        "... The crew of the USS Discovery discovered many discoveries.\n",
        "... Discovering is what explorers do.\"\"\""
      ],
      "metadata": {
        "id": "kHxuha3eyWdU"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can stem the words in that string, we need to separate all the words in it."
      ],
      "metadata": {
        "id": "9nOxpnizzVEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(string_for_stemming)"
      ],
      "metadata": {
        "id": "zEoY7Xxmya7M"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you have a list of all the tokenized words in the string."
      ],
      "metadata": {
        "id": "9wENGhyrdPm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6RB0UlYycuk",
        "outputId": "b194a7a8-4396-4f49-d51f-339095d05af2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'crew',\n",
              " 'of',\n",
              " 'the',\n",
              " 'USS',\n",
              " 'Discovery',\n",
              " 'discovered',\n",
              " 'many',\n",
              " 'discoveries',\n",
              " '.',\n",
              " 'Discovering',\n",
              " 'is',\n",
              " 'what',\n",
              " 'explorers',\n",
              " 'do',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We created a list of the stemmed versions of the words in words by using stemmer.stem() in a list comprehension."
      ],
      "metadata": {
        "id": "T8pvSziWy8Os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_words = [stemmer.stem(word) for word in words]"
      ],
      "metadata": {
        "id": "8Xn-FQVqyock"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOAwwmkDysjD",
        "outputId": "a1cf56f0-4007-4ba6-8c3a-b8307d247bce"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'crew',\n",
              " 'of',\n",
              " 'the',\n",
              " 'uss',\n",
              " 'discoveri',\n",
              " 'discov',\n",
              " 'mani',\n",
              " 'discoveri',\n",
              " '.',\n",
              " 'discov',\n",
              " 'is',\n",
              " 'what',\n",
              " 'explor',\n",
              " 'do',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tagging Parts of Speech:** Part of speech is a grammatical term that deals with the roles words play when we use them together in sentences. Tagging parts of speech, or POS tagging, is the task of labeling the words in our text according to their part of speech."
      ],
      "metadata": {
        "id": "aWvky_jo4usF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how to import the relevant parts of NLTK in order to tag parts of speech."
      ],
      "metadata": {
        "id": "2HMvCS7m38uk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "5s10IWoC0piT"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sagan_quote = \"\"\"\n",
        "... If you wish to make an apple pie from scratch,\n",
        "... you must first invent the universe.\"\"\""
      ],
      "metadata": {
        "id": "Q43RiEp20rqz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use word_tokenize to separate the words in that string and store them in a list."
      ],
      "metadata": {
        "id": "WBlY_B4D3tiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_sagan_quote = word_tokenize(sagan_quote)"
      ],
      "metadata": {
        "id": "6PL9lZb40wEE"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us call nltk.pos_tag() on our new list of words."
      ],
      "metadata": {
        "id": "pw6FICew3c1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.pos_tag(words_in_sagan_quote)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08IKqykc09KM",
        "outputId": "70b4a403-82df-4466-8a48-de750957966d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('If', 'IN'),\n",
              " ('you', 'PRP'),\n",
              " ('wish', 'VBP'),\n",
              " ('to', 'TO'),\n",
              " ('make', 'VB'),\n",
              " ('an', 'DT'),\n",
              " ('apple', 'NN'),\n",
              " ('pie', 'NN'),\n",
              " ('from', 'IN'),\n",
              " ('scratch', 'NN'),\n",
              " (',', ','),\n",
              " ('you', 'PRP'),\n",
              " ('must', 'MD'),\n",
              " ('first', 'VB'),\n",
              " ('invent', 'VB'),\n",
              " ('the', 'DT'),\n",
              " ('universe', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see all the words in the quote are now in a separate tuple, with a tag that represents their part of speech."
      ],
      "metadata": {
        "id": "ibnjiERz3Iam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can get a list of tags and their meanings in the following way."
      ],
      "metadata": {
        "id": "sjCwtInSd9uT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('tagsets')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExOW3AG31Kzb",
        "outputId": "53c4d7f6-01c9-4d9a-f407-97449c1e9e1f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can make a string to hold an excerpt from this poem."
      ],
      "metadata": {
        "id": "c3HnpGgU2lxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jabberwocky_excerpt = \"\"\"\n",
        "... 'Twas brillig, and the slithy toves did gyre and gimble in the wabe:\n",
        "... all mimsy were the borogoves, and the mome raths outgrabe.\"\"\""
      ],
      "metadata": {
        "id": "xi-LpBgX1iPT"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use word_tokenize to separate the words in the excerpt and store them in a list."
      ],
      "metadata": {
        "id": "aoROz0q22Qr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " words_in_excerpt = word_tokenize(jabberwocky_excerpt)"
      ],
      "metadata": {
        "id": "xerXmu6v1j8j"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us call nltk.pos_tag() on our new list of words."
      ],
      "metadata": {
        "id": "4I2P9vgeeh1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.pos_tag(words_in_excerpt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-eUJuiw1oji",
        "outputId": "bf697cfc-8b0c-4c5e-f985-9e07465e31ff"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"'Twas\", 'CD'),\n",
              " ('brillig', 'NN'),\n",
              " (',', ','),\n",
              " ('and', 'CC'),\n",
              " ('the', 'DT'),\n",
              " ('slithy', 'JJ'),\n",
              " ('toves', 'NNS'),\n",
              " ('did', 'VBD'),\n",
              " ('gyre', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('gimble', 'JJ'),\n",
              " ('in', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('wabe', 'NN'),\n",
              " (':', ':'),\n",
              " ('all', 'DT'),\n",
              " ('mimsy', 'NNS'),\n",
              " ('were', 'VBD'),\n",
              " ('the', 'DT'),\n",
              " ('borogoves', 'NNS'),\n",
              " (',', ','),\n",
              " ('and', 'CC'),\n",
              " ('the', 'DT'),\n",
              " ('mome', 'JJ'),\n",
              " ('raths', 'NNS'),\n",
              " ('outgrabe', 'RB'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, accepted English words like 'and' and 'the' were correctly tagged as a conjunction and a determiner, respectively. The gibberish word 'slithy' was tagged as an adjective; that is what a human English speaker would probably assume from the context of the poem as well."
      ],
      "metadata": {
        "id": "NmfXPhSt2BGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatizing:** Lemmatizing reduces words to their core meaning, but it will give us a complete English word that makes sense on its own instead of just a fragment of a word."
      ],
      "metadata": {
        "id": "fdt3r98U58yR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us see how to import the relevant parts of NLTK in order to start lemmatizing."
      ],
      "metadata": {
        "id": "CbYM9He86dX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "oHRkV46T5Ci1"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can create a lemmatizer to use."
      ],
      "metadata": {
        "id": "6lOm5m3R6px-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "OmIVqV3t5KPZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also lemmatize a plural noun."
      ],
      "metadata": {
        "id": "H22xCd1W680x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "lemmatizer.lemmatize(\"scarves\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "PSske8045MGJ",
        "outputId": "c7de0f4b-f47f-4a97-ba94-c46de1fa0512"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'scarf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"scarves\" gave us 'scarf',  and this is more meaningful than what we got with the stemmer, which is 'scarv'. Next, create a string with more than one word to lemmatize:"
      ],
      "metadata": {
        "id": "cJU_T4BJ7cwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string_for_lemmatizing = \"The friends of DeSoto love scarves.\""
      ],
      "metadata": {
        "id": "4wCucxLw5Y1j"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can tokenize that string by word."
      ],
      "metadata": {
        "id": "zh1VwJyT7quQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(string_for_lemmatizing)"
      ],
      "metadata": {
        "id": "SH0ouArm5bcb"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is our list of words:"
      ],
      "metadata": {
        "id": "XF8M5CIa7zVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8nXUmLQ5gWD",
        "outputId": "248ce44f-54d8-48f4-fbce-a22c2854de6b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'friends', 'of', 'DeSoto', 'love', 'scarves', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have created a list containing all the words in words after they’ve been lemmatized."
      ],
      "metadata": {
        "id": "JzWqYmid8Eat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]"
      ],
      "metadata": {
        "id": "eAw3z_5H5mm1"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the list we got."
      ],
      "metadata": {
        "id": "KQo0Sm2Y8SQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtzWVDXT5qjN",
        "outputId": "134037be-ef7d-4672-f4f5-5d1f9b1a004d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'friend', 'of', 'DeSoto', 'love', 'scarf', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us try lemmatizing \"worst\"."
      ],
      "metadata": {
        "id": "n2zoj_Yv8jpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"worst\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_Meh4c0S5u_l",
        "outputId": "9f3033fa-0c7a-45fa-f03b-34709504c9ed"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'worst'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got the result 'worst' because lemmatizer.lemmatize() assumed that \"worst\" was a noun. We can make it clear that we want \"worst\" to be an adjective."
      ],
      "metadata": {
        "id": "ySuMJ35I89px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"worst\", pos=\"a\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Zc-1_yWc5zFl",
        "outputId": "c1d5459c-092a-4a98-95d5-454dd8743697"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bad'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The default parameter for pos is 'n' for noun, but we made sure that \"worst\" was treated as an adjective by adding the parameter pos=\"a\". As a result, we got 'bad', which looks very different from our original word and is nothing like what we could get if we were stemming. We are getting this because \"worst\" is the superlative form of the adjective 'bad', and lemmatizing can reduce superlatives as well as comparatives to their lemmas."
      ],
      "metadata": {
        "id": "6oFq8IMO9nbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chunking:** While tokenizing allows us to identify words and sentences, chunking allows us to identify phrases."
      ],
      "metadata": {
        "id": "JklQNVTP__-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how to import the relevant parts of NLTK in order to chunk."
      ],
      "metadata": {
        "id": "5EvtdM7nAWIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "V-KURJH0-FNM"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can chunk, we need to make sure that the parts of speech in our text are tagged, so create a string for POS tagging."
      ],
      "metadata": {
        "id": "71lp1CX2Ar62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lotr_quote = \"It's a dangerous business, Frodo, going out your door.\""
      ],
      "metadata": {
        "id": "dHl-f6dM-Iwd"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can tokenize that string by word."
      ],
      "metadata": {
        "id": "KlOPW9MeAyXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_lotr_quote = word_tokenize(lotr_quote)"
      ],
      "metadata": {
        "id": "OJzq2u2S-NKm"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_lotr_quote"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8HoiiG_-Qv_",
        "outputId": "9c9ee491-390c-421e-d55b-7aaf096e10c3"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It',\n",
              " \"'s\",\n",
              " 'a',\n",
              " 'dangerous',\n",
              " 'business',\n",
              " ',',\n",
              " 'Frodo',\n",
              " ',',\n",
              " 'going',\n",
              " 'out',\n",
              " 'your',\n",
              " 'door',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next step, we can tag those words by part of speech."
      ],
      "metadata": {
        "id": "4f7ij33hA_-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"averaged_perceptron_tagger\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7FlJkWM-V82",
        "outputId": "6bbd01eb-8942-409c-e3a5-8bacd63c9ad7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lotr_pos_tags = nltk.pos_tag(words_in_lotr_quote)"
      ],
      "metadata": {
        "id": "gvcF7qIL-Z6X"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lotr_pos_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvOepZw5-dph",
        "outputId": "decead82-00d1-4ca2-a146-e26add666677"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('It', 'PRP'),\n",
              " (\"'s\", 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('dangerous', 'JJ'),\n",
              " ('business', 'NN'),\n",
              " (',', ','),\n",
              " ('Frodo', 'NNP'),\n",
              " (',', ','),\n",
              " ('going', 'VBG'),\n",
              " ('out', 'RP'),\n",
              " ('your', 'PRP$'),\n",
              " ('door', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a list of tuples of all the words in the quote, along with their POS tag. In order to chunk, we first need to understand a chunk grammar. A **chunk grammar** is a combination of rules on how sentences should be chunked. It often uses regular expressions, or regexes."
      ],
      "metadata": {
        "id": "EgFHKuFHC6Xg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us create a chunk grammar with one regular expression rule."
      ],
      "metadata": {
        "id": "aKsIcor6D5eO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\""
      ],
      "metadata": {
        "id": "PCjRQ2xC-nF5"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, NP stands for noun phrase. According to the rule we created, our chunks:\n",
        "\n",
        "1. Start with an optional (?) determiner ('DT')\n",
        "2. Can have any number (*) of adjectives (JJ)\n",
        "3. End with a noun (< NN >)"
      ],
      "metadata": {
        "id": "Cp9p1nazIqzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We created a chunk parser with this grammar."
      ],
      "metadata": {
        "id": "Cp7JJ-f7fvkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_parser = nltk.RegexpParser(grammar)"
      ],
      "metadata": {
        "id": "Uu7tQriT-rW5"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can try it out with our quote."
      ],
      "metadata": {
        "id": "rhLJHcpqgGG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree = chunk_parser.parse(lotr_pos_tags)\n"
      ],
      "metadata": {
        "id": "s62Yvvqd-s-V"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The visual representation can be done using tree.draw()."
      ],
      "metadata": {
        "id": "_calq1aGhURU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chinking:** Chinking is used together with chunking, but while chunking is used to include a pattern, chinking is used to exclude a pattern."
      ],
      "metadata": {
        "id": "PqvclEk6jpcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lotr_pos_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYdSNr4phRJ4",
        "outputId": "cd24f9ff-89f0-4853-9a75-602fd71379e6"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('It', 'PRP'),\n",
              " (\"'s\", 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('dangerous', 'JJ'),\n",
              " ('business', 'NN'),\n",
              " (',', ','),\n",
              " ('Frodo', 'NNP'),\n",
              " (',', ','),\n",
              " ('going', 'VBG'),\n",
              " ('out', 'RP'),\n",
              " ('your', 'PRP$'),\n",
              " ('door', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grammar = \"\"\"\n",
        "... Chunk: {<.*>+}\n",
        "...        }<JJ>{\"\"\""
      ],
      "metadata": {
        "id": "Hl2KLFwch9Vd"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first rule of our grammar is {<. * >+}. This rule has curly braces that face inward ({}) because it is used to determine what patterns we want to include in our chunks. In this case, we want to include everything: <.*>+.\n",
        "\n",
        "The second rule of your grammar is }< JJ >{. This rule has curly braces that face outward (}{) because it is used to determine what patterns you want to exclude in your chunks. In this case, you want to exclude adjectives: < JJ >."
      ],
      "metadata": {
        "id": "68nt5pcOkBcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_parser = nltk.RegexpParser(grammar)"
      ],
      "metadata": {
        "id": "oF6J1OOviDj_"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree = chunk_parser.parse(lotr_pos_tags)\n"
      ],
      "metadata": {
        "id": "Ez7-f2iDiF2v"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The visual representation can be done using tree.draw()."
      ],
      "metadata": {
        "id": "myMj59jIjhg9"
      }
    }
  ]
}